# ğŸ”® Prediction Service - HÆ°á»›ng dáº«n Setup (Architecture má»›i)

## ğŸ¯ Architecture

### TrÆ°á»›c (cháº¡y prediction trong Docker):

```
Web App â†’ Docker Server (Flask + PyTorch) â†’ Save CSV + Predict
                âŒ Náº·ng, pháº£i cÃ i PyTorch trong container
                âŒ Build lÃ¢u (5-10 phÃºt)
                âŒ Prediction cháº­m
```

### Sau (Prediction Service riÃªng biá»‡t):

```
Web App â†’ Docker Server (Flask only) â†’ Save CSV
                â†“
         HTTP Request
                â†“
         Prediction Service (Local - FastAPI)
         âœ… Cháº¡y trÃªn host machine
         âœ… PyTorch Ä‘Ã£ cÃ³ sáºµn
         âœ… Prediction nhanh
         âœ… Dá»… debug vÃ  restart
```

---

## ğŸ“¦ Setup Instructions

### BÆ°á»›c 1: Install Prediction Service dependencies

```bash
cd PBL4-Network-Traffic-Prediction
pip install -r requirements-prediction-service.txt
```

**Dependencies:**

- FastAPI (Web framework)
- Uvicorn (ASGI server)
- PyTorch (Ä‘Ã£ cÃ³ tá»« training)
- NumPy, Pandas, scikit-learn

### BÆ°á»›c 2: Start Prediction Service

#### Windows:

```bash
cd PBL4-Network-Traffic-Prediction
start-prediction-service.bat
```

#### Linux/Mac:

```bash
cd PBL4-Network-Traffic-Prediction
chmod +x start-prediction-service.sh
./start-prediction-service.sh
```

#### Manual:

```bash
cd PBL4-Network-Traffic-Prediction
python prediction_service.py --port 5000
```

**Expected output:**

```
======================================
ğŸš€ PBL4 Prediction Service
======================================
Host: 0.0.0.0
Port: 5000
URL:  http://localhost:5000
======================================

INFO:     Started server process
INFO:     Uvicorn running on http://0.0.0.0:5000
```

### BÆ°á»›c 3: Rebuild Docker (khÃ´ng cáº§n PyTorch ná»¯a)

```bash
cd SAGSINs-System/docker
docker-compose down
docker-compose build sagsins-server  # âœ… Nhanh hÆ¡n nhiá»u!
docker-compose up -d
```

â±ï¸ **LÆ°u Ã½**: Build chá»‰ máº¥t ~30 giÃ¢y (khÃ´ng pháº£i 5-10 phÃºt nhÆ° trÆ°á»›c)

### BÆ°á»›c 4: Test

#### Test Prediction Service trá»±c tiáº¿p:

```bash
curl http://localhost:5000/health
```

Expected:

```json
{
  "status": "healthy",
  "predictor": "loaded",
  "models": {
    "vae": true,
    "lstm": true
  }
}
```

#### Test qua simulator:

1. Send packet tá»« Web App
2. Check Docker logs: `docker logs -f sagsins-server`
3. Sáº½ tháº¥y:

```
ğŸ“¦ Received packet: SATELLITE_01 -> GROUND_GATEWAY_01
ğŸ”® Calling prediction service at http://host.docker.internal:5000...
âœ… Received prediction from service
```

---

## ğŸ”§ Configuration

### Environment Variables (trong docker-compose.yml):

```yaml
environment:
  # Prediction Service URL
  - PREDICTION_SERVICE_URL=http://host.docker.internal:5000

  # Enable/disable predictions
  - PREDICTION_ENABLED=true

  # CSV path trÃªn host machine
  - HOST_TRAFFIC_CSV=D:/HuyCoding/PBL4/SAGSINs-System/docker/data/traffic_data.csv
```

### Táº¯t Prediction (náº¿u cáº§n):

```yaml
environment:
  - PREDICTION_ENABLED=false
```

---

## ğŸ“Š API Endpoints

### Prediction Service (Port 5000)

#### `GET /`

Root endpoint - service info

#### `GET /health`

Health check

Response:

```json
{
  "status": "healthy",
  "predictor": "loaded",
  "models": {
    "vae": true,
    "lstm": true
  }
}
```

#### `POST /predict`

Run prediction

Request:

```json
{
  "csv_path": "D:/HuyCoding/PBL4/SAGSINs-System/docker/data/traffic_data.csv",
  "use_latest": true
}
```

Response:

```json
{
  "status": "success",
  "prediction": {
    "link_id": "LINK_SPACE_GROUND_01",
    "timestamp": "2025-11-07T10:30:00",
    "vae": {
      "utilization": 0.732,
      "utilization_percent": 73.2,
      "status": "MEDIUM"
    },
    "lstm": {
      "utilization": 0.745,
      "utilization_percent": 74.5,
      "status": "MEDIUM"
    },
    "average": {
      "utilization": 0.7385,
      "utilization_percent": 73.85,
      "status": "MEDIUM"
    }
  }
}
```

---

## ğŸ§ª Testing

### Test Script (Python):

```python
import requests

# Test health
response = requests.get('http://localhost:5000/health')
print(response.json())

# Test prediction
response = requests.post(
    'http://localhost:5000/predict',
    json={
        'csv_path': 'D:/HuyCoding/PBL4/SAGSINs-System/docker/data/traffic_data.csv',
        'use_latest': True
    }
)
print(response.json())
```

### Test vá»›i curl:

```bash
# Health check
curl http://localhost:5000/health

# Prediction
curl -X POST http://localhost:5000/predict \
  -H "Content-Type: application/json" \
  -d '{"csv_path":"D:/HuyCoding/PBL4/SAGSINs-System/docker/data/traffic_data.csv","use_latest":true}'
```

---

## ğŸ” Troubleshooting

### âŒ "Cannot connect to prediction service"

**Triá»‡u chá»©ng:**

```
âš ï¸  Cannot connect to prediction service at http://host.docker.internal:5000
```

**NguyÃªn nhÃ¢n**: Prediction service chÆ°a cháº¡y

**Giáº£i phÃ¡p:**

```bash
cd PBL4-Network-Traffic-Prediction
python prediction_service.py --port 5000
```

---

### âŒ "host.docker.internal not resolved"

**Triá»‡u chá»©ng:**

```
Cannot resolve host.docker.internal
```

**NguyÃªn nhÃ¢n**: Docker khÃ´ng há»— trá»£ `host.docker.internal`

**Giáº£i phÃ¡p:**

**Option 1**: DÃ¹ng host IP

```yaml
environment:
  - PREDICTION_SERVICE_URL=http://192.168.1.100:5000 # Your host IP
```

**Option 2**: DÃ¹ng `network_mode: host` (Linux only)

```yaml
services:
  sagsins-server:
    network_mode: host
```

---

### âŒ "Model not found"

**Triá»‡u chá»©ng:**

```
FileNotFoundError: models/simple_vae_best.pth
```

**NguyÃªn nhÃ¢n**: Models chÆ°a Ä‘Æ°á»£c train

**Giáº£i phÃ¡p:**

```bash
cd PBL4-Network-Traffic-Prediction
python train_vae_simple.py
python train_lstm.py
```

---

### âš ï¸ "Prediction timeout"

**Triá»‡u chá»©ng:**

```
âš ï¸  Prediction service timeout
```

**NguyÃªn nhÃ¢n**: Prediction máº¥t quÃ¡ 10 giÃ¢y

**Giáº£i phÃ¡p**: TÄƒng timeout trong `app.py`

```python
response = requests.post(
    ...,
    timeout=30  # TÄƒng lÃªn 30 giÃ¢y
)
```

---

## ğŸš€ Workflow hoÃ n chá»‰nh

### 1. Start Prediction Service (Local)

```bash
cd PBL4-Network-Traffic-Prediction
python prediction_service.py --port 5000
```

Keep this running in terminal 1.

### 2. Start Docker Containers

```bash
cd SAGSINs-System/docker
docker-compose up -d
```

### 3. Start Web App

```bash
cd SAGSINs-System/wep-app/frontend
npm run dev
```

### 4. Send Packet & Watch Logs

Terminal 1 (Prediction Service):

```
ğŸ”® Running prediction for LINK_SPACE_GROUND_01...
   ğŸ“ˆ VAE:  73.20% (MEDIUM)
   ğŸ“ˆ LSTM: 74.50% (MEDIUM)
   ğŸ“Š AVG:  73.85% (MEDIUM)
âœ… Prediction complete
```

Terminal 2 (Docker Server):

```bash
docker logs -f sagsins-server

# Output:
ğŸ“¦ Received packet: SATELLITE_01 -> GROUND_GATEWAY_01
ğŸ”® Calling prediction service...
âœ… Received prediction from service
```

---

## âœ… Advantages

### âœ¨ So vá»›i architecture cÅ©:

| TiÃªu chÃ­             | CÅ© (In Docker) | Má»›i (Separate Service) |
| -------------------- | -------------- | ---------------------- |
| **Build time**       | 5-10 phÃºt      | 30 giÃ¢y                |
| **Container size**   | ~2GB           | ~200MB                 |
| **Prediction speed** | Cháº­m           | Nhanh                  |
| **Debug**            | KhÃ³            | Dá»…                     |
| **Restart**          | Pháº£i rebuild   | Chá»‰ restart service    |
| **Resource**         | Náº·ng           | Nháº¹                    |

### ğŸ¯ Benefits:

1. âœ… **KhÃ´ng cáº§n rebuild Docker** khi thay Ä‘á»•i model code
2. âœ… **Debug dá»… dÃ ng** - chá»‰ cáº§n restart Python script
3. âœ… **Prediction nhanh hÆ¡n** - cháº¡y trá»±c tiáº¿p trÃªn host
4. âœ… **Äá»™c láº­p** - cÃ³ thá»ƒ dÃ¹ng cho nhiá»u clients khÃ¡c
5. âœ… **Scale dá»…** - cÃ³ thá»ƒ deploy service lÃªn cloud riÃªng

---

## ğŸ“ˆ Next Steps (Optional)

### 1. Deploy Prediction Service lÃªn Cloud

```bash
# Example: Deploy to Heroku
heroku create pbl4-prediction-service
git push heroku main
```

Update docker-compose.yml:

```yaml
environment:
  - PREDICTION_SERVICE_URL=https://pbl4-prediction-service.herokuapp.com
```

### 2. Add Caching

```python
from functools import lru_cache

@lru_cache(maxsize=100)
def predict_cached(csv_hash):
    # Cache predictions for same traffic data
    pass
```

### 3. Add Authentication

```python
from fastapi.security import HTTPBearer

@app.post("/predict")
def predict(request: PredictRequest, token: str = Depends(HTTPBearer())):
    # Verify token
    pass
```

---

## ğŸ“š Files Created/Modified

### New Files:

- âœ… `PBL4-Network-Traffic-Prediction/prediction_service.py`
- âœ… `PBL4-Network-Traffic-Prediction/requirements-prediction-service.txt`
- âœ… `PBL4-Network-Traffic-Prediction/start-prediction-service.bat`
- âœ… `PBL4-Network-Traffic-Prediction/start-prediction-service.sh`

### Modified Files:

- âœ… `SAGSINs-System/docker/server/app.py` - Call HTTP API thay vÃ¬ local prediction
- âœ… `SAGSINs-System/docker/server/requirements.txt` - Bá» PyTorch
- âœ… `SAGSINs-System/docker/docker-compose.yml` - Add env vars, remove PBL4 mount

---

**ğŸ‰ Architecture má»›i: Nháº¹ hÆ¡n, nhanh hÆ¡n, dá»… maintain hÆ¡n!**

---

**Version**: 2.0 (Separate Prediction Service)  
**Date**: 2025-11-07  
**Author**: PBL4 Team

# PBL4 ‚Äì Network Traffic Prediction

## H∆Ø·ªöNG D·∫™N CH·∫†Y (Windows / PowerShell)

1. T·∫°o m√¥i tr∆∞·ªùng ·∫£o v√† c√†i dependencies

---

python -m venv venv
.\venv\Scripts\activate
pip install -r requirements.txt

2. Ch·∫°y ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu (Preprocessing)

---

python preprocessing.py

3. Pipeline ch√≠nh ƒë√£ l√†m g√¨

---

- Resample d·ªØ li·ªáu theo t·ª´ng link, c·ª≠a s·ªï 10 gi√¢y
  - bytes_sent -> SUM
  - bitrate_bps, rtt, loss_rate, jitter, latency -> MEAN
  - capacity_bps -> LAST / FFILL
- T·∫°o features:
  - hour, is_weekend
  - utilization (t√≠nh theo bƒÉng th√¥ng, chu·∫©n h√≥a)
  - throughput_mbps
  - quality_score (d·ª±a tr√™n loss + jitter)
  - efficiency = utilization \* quality_score
- Pivot d·ªØ li·ªáu th√†nh snapshot d·∫°ng "timestamp x link (wide)"
  - d√πng chung cho LSTM & VAE
  - gi·ªØ th·ª© t·ª± link c·ªë ƒë·ªãnh (link_index.json)
- Chu·∫©n h√≥a d·ªØ li·ªáu:
  - Fit MinMaxScaler theo t·ª´ng feature
  - Ch·ªâ fit tr√™n TRAIN, sau ƒë√≥ transform to√†n b·ªô
- Sinh chu·ªói LSTM:
  - sequence_length = 24 (24 b∆∞·ªõc = 4 ph√∫t n·∫øu b∆∞·ªõc 10s)
  - horizon = 1 (d·ª± b√°o b∆∞·ªõc k·∫ø ti·∫øp)
- Chia train/val/test theo th·ªùi gian (kh√¥ng shuffle)
  - Train = 70%
  - Val = 15%
  - Test = 15%

4. C√°c file k·∫øt qu·∫£ quan tr·ªçng

---

**_D·ªØ li·ªáu cho LSTM_**

- data/X_train.npy, data/y_train.npy
- data/X_val.npy, data/y_val.npy
- data/X_test.npy, data/y_test.npy

**_D·ªØ li·ªáu cho VAE_**

- data/vae_snapshots.npy : to√†n b·ªô snapshot ƒë√£ scale
- data/vae_columns.json : t√™n c·ªôt (feature x link) theo th·ª© t·ª± c·ªë ƒë·ªãnh

**_T√°i l·∫≠p / Inference_**

- data/features.json : danh s√°ch feature ƒë√£ ch·ªçn
- data/link_index.json : th·ª© t·ª± link c·ªë ƒë·ªãnh
- data/timestamp_splits.json: th√¥ng tin chia train/val/test (theo th·ªùi gian)
- models/wide_scalers.pkl : scaler MinMax cho t·ª´ng feature, fit tr√™n train

**_Ph√¢n t√≠ch ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu_**

- data/missing_mask.npy : ma tr·∫≠n True/False c√πng shape v·ªõi vae_snapshots
  - True = gi√° tr·ªã g·ªëc b·ªã thi·∫øu (NaN tr∆∞·ªõc khi interpolate/ffill)
  - D√πng ƒë·ªÉ:
    - Ki·ªÉm tra ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu, xem link n√†o hay thi·∫øu
    - Thi·∫øt k·∫ø masked loss (kh√¥ng ph·∫°t n·∫∑ng t·∫°i ƒëi·ªÉm thi·∫øu)
    - Debug model (n·∫øu d·ª± ƒëo√°n k√©m ·ªü ƒëo·∫°n d·ªØ li·ªáu thi·∫øu nhi·ªÅu)

**_Tham chi·∫øu_**

- data/traffic_processed.csv: d·ªØ li·ªáu ƒë√£ resample + feature engineering, tr∆∞·ªõc khi pivot

xem 4 ·∫£nh ƒë·ªÉ hi·ªÉu d·ªØ li·ªáu m√† lstm vs vae nh·∫≠n v√†o nh∆∞ th·∫ø n√†o nha
## 4. Ch·∫°y LSTM Baseline Model

---

```bash
python lstm_baseline.py
```

### üìä **Chi ti·∫øt t·ª´ng b∆∞·ªõc c·ªßa LSTM Baseline:**

#### **4.1. Kh·ªüi t·∫°o v√† Setup (d√≤ng 400-415)**
```python
# T·∫°o th∆∞ m·ª•c results/ v√† models/
# Ki·ªÉm tra GPU/CPU v√† ch·ªçn device ph√π h·ª£p
# In th√¥ng tin device ƒëang s·ª≠ d·ª•ng
```

#### **4.2. Load Data (d√≤ng 417-418)**
```python
# Load c√°c file .npy ƒë√£ ƒë∆∞·ª£c preprocessing:
# - X_train.npy: (6024, 24, 72) - 6024 sequences, 24 timesteps, 72 features
# - y_train.npy: (6024, 12) - 6024 targets, 12 links (utilization)
# - X_val.npy, y_val.npy: (1296, 24, 72), (1296, 12)
# - X_test.npy, y_test.npy: (1296, 24, 72), (1296, 12)
# - features.json: 6 features ƒë∆∞·ª£c ch·ªçn
# - link_index.json: 12 t√™n links theo th·ª© t·ª±
```

#### **4.3. T·∫°o DataLoaders (d√≤ng 420-424)**
```python
# Convert numpy arrays th√†nh PyTorch tensors
# T·∫°o DataLoader v·ªõi batch_size:
# - GPU: batch_size = 64
# - CPU: batch_size = 32
# - shuffle=True cho train, False cho val/test
```

#### **4.4. Model Configuration (d√≤ng 426-443)**
```python
# input_size = 72 (6 features √ó 12 links)
# output_size = 12 (12 links utilization)
# sequence_length = 24 (4 ph√∫t v·ªõi 10s intervals)
# LSTM Model:
#   - hidden_size = 128
#   - num_layers = 3
#   - dropout = 0.3
#   - Fully connected: 128 ‚Üí 64 ‚Üí 12
```

#### **4.5. Model Architecture (d√≤ng 16-69)**
```python
# LSTMModel.forward():
# 1. Input: [batch_size, 24, 72]
# 2. LSTM layers: 3 layers, 128 hidden units
# 3. L·∫•y output cu·ªëi c√πng: [batch_size, 128]
# 4. FC layers: 128 ‚Üí 64 ‚Üí 12
# 5. Output: [batch_size, 12] (utilization cho 12 links)
```

#### **4.6. Training Process (d√≤ng 453-459)**
```python
# LSTMTrainer.train():
# - Optimizer: Adam (lr=0.001, weight_decay=1e-5)
# - Loss: MSE (Mean Squared Error)
# - Scheduler: ReduceLROnPlateau (patience=5, factor=0.5)
# - Epochs: 200 (v·ªõi early stopping)
# - Patience: 15 epochs
# - Gradient clipping: max_norm=1.0
```

#### **4.7. Training Loop (d√≤ng 127-169)**
```python
# M·ªói epoch:
# 1. train_epoch(): Forward pass, backward pass, update weights
# 2. validate_epoch(): Forward pass tr√™n validation set
# 3. Learning rate scheduling d·ª±a tr√™n validation loss
# 4. Early stopping n·∫øu validation loss kh√¥ng c·∫£i thi·ªán
# 5. Save best model khi validation loss gi·∫£m
# 6. Print progress m·ªói 10 epochs
```

#### **4.8. Evaluation (d√≤ng 464-486)**
```python
# LSTMEvaluator.predict():
# 1. Load best model
# 2. Forward pass tr√™n test set
# 3. Collect predictions v√† true values
# 4. Calculate metrics: MSE, RMSE, MAE, R¬≤
# 5. Per-link performance analysis
```

#### **4.9. Metrics Calculation (d√≤ng 197-224)**
```python
# Overall metrics:
# - MSE: Mean Squared Error
# - RMSE: Root Mean Squared Error
# - MAE: Mean Absolute Error
# - R¬≤: R-squared (coefficient of determination)

# Per-link metrics:
# - MSE v√† MAE cho t·ª´ng link ri√™ng bi·ªát
# - So s√°nh performance gi·ªØa c√°c lo·∫°i link
```

#### **4.10. Visualizations (d√≤ng 226-303)**
```python
# 1. plot_training_curves():
#    - Training loss vs Validation loss
#    - Smoothed curves v·ªõi rolling window

# 2. plot_results():
#    - So s√°nh predicted vs true values
#    - Plot 4 links ƒë·∫ßu ti√™n (2x2 subplot)
#    - 200 samples ƒë·∫ßu ti√™n

# 3. plot_scatter():
#    - Scatter plot predicted vs true
#    - Perfect prediction line (y=x)
#    - Flatten t·∫•t c·∫£ links th√†nh 1D
```

#### **4.11. Results Saving (d√≤ng 488-522)**
```python
# L∆∞u v√†o results/:
# - lstm_results.json: Detailed metrics v√† config
# - training_curves.png: Loss curves
# - lstm_results.png: Prediction plots
# - lstm_scatter.png: Scatter plot

# L∆∞u v√†o models/:
# - best_lstm_model.pth: Best trained model weights
```

### üéØ **K·∫øt qu·∫£ mong ƒë·ª£i:**

#### **Input c·ªßa LSTM:**
- **X**: (batch_size, 24, 72) - 24 timesteps c·ªßa 6 features cho 12 links
- **Features**: utilization, bitrate_bps, loss_rate, jitter_milliseconds, rtt_milliseconds, capacity_bps
- **Links**: 12 SAGSINs links (space, air, ground, sea)

#### **Output c·ªßa LSTM:**
- **y_pred**: (batch_size, 12) - Utilization prediction cho 12 links
- **Range**: [0, 1] (ƒë√£ ƒë∆∞·ª£c normalize)
- **Horizon**: 1 b∆∞·ªõc ti·∫øp theo (10 gi√¢y sau)

#### **Performance Metrics:**
- **MSE**: Th·∫•p h∆°n = t·ªët h∆°n
- **RMSE**: CƒÉn b·∫≠c 2 c·ªßa MSE
- **MAE**: Trung b√¨nh absolute error
- **R¬≤**: G·∫ßn 1 = t·ªët h∆°n (perfect = 1.0)

#### **Files ƒë∆∞·ª£c t·∫°o:**